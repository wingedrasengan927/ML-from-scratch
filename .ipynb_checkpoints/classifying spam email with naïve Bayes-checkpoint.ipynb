{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare: tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a string\n",
    "mySent='This book is the best book on Python or M.L. I have ever laid eyes upon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's split this string\n",
    "mySent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to convert the string into a word vector, you need to split on punctuations also\n",
    "# we can use the regular expression class \\W to split on anything that isn't a word or number\n",
    "import re \n",
    "listofTokens = re.split('\\W+', mySent)\n",
    "listofTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we need to remove empty strings\n",
    "listofTokens = [x for x in listofTokens if len(x)>0]\n",
    "listofTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to convert everything into lower case\n",
    "listofTokens = [x.lower() for x in listofTokens]\n",
    "listofTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Since',\n",
       " 'you',\n",
       " 'are',\n",
       " 'an',\n",
       " 'owner',\n",
       " 'of',\n",
       " 'at',\n",
       " 'least',\n",
       " 'one',\n",
       " 'Google',\n",
       " 'Groups',\n",
       " 'group',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'customized',\n",
       " 'welcome',\n",
       " 'message',\n",
       " 'pages',\n",
       " 'or',\n",
       " 'files',\n",
       " 'we',\n",
       " 'are',\n",
       " 'writing',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'you',\n",
       " 'that',\n",
       " 'we',\n",
       " 'will',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'be',\n",
       " 'supporting',\n",
       " 'these',\n",
       " 'features',\n",
       " 'starting',\n",
       " 'February',\n",
       " '2011',\n",
       " 'We',\n",
       " 'made',\n",
       " 'this',\n",
       " 'decision',\n",
       " 'so',\n",
       " 'that',\n",
       " 'we',\n",
       " 'can',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'improving',\n",
       " 'the',\n",
       " 'core',\n",
       " 'functionalities',\n",
       " 'of',\n",
       " 'Google',\n",
       " 'Groups',\n",
       " 'mailing',\n",
       " 'lists',\n",
       " 'and',\n",
       " 'forum',\n",
       " 'discussions',\n",
       " 'Instead',\n",
       " 'of',\n",
       " 'these',\n",
       " 'features',\n",
       " 'we',\n",
       " 'encourage',\n",
       " 'you',\n",
       " 'to',\n",
       " 'use',\n",
       " 'products',\n",
       " 'that',\n",
       " 'are',\n",
       " 'designed',\n",
       " 'specifically',\n",
       " 'for',\n",
       " 'file',\n",
       " 'storage',\n",
       " 'and',\n",
       " 'page',\n",
       " 'creation',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Google',\n",
       " 'Docs',\n",
       " 'and',\n",
       " 'Google',\n",
       " 'Sites',\n",
       " 'For',\n",
       " 'example',\n",
       " 'you',\n",
       " 'can',\n",
       " 'easily',\n",
       " 'create',\n",
       " 'your',\n",
       " 'pages',\n",
       " 'on',\n",
       " 'Google',\n",
       " 'Sites',\n",
       " 'and',\n",
       " 'share',\n",
       " 'the',\n",
       " 'site',\n",
       " 'http',\n",
       " 'www',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'sites',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'py',\n",
       " 'hl',\n",
       " 'en',\n",
       " 'answer',\n",
       " '174623',\n",
       " 'with',\n",
       " 'the',\n",
       " 'members',\n",
       " 'of',\n",
       " 'your',\n",
       " 'group',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'store',\n",
       " 'your',\n",
       " 'files',\n",
       " 'on',\n",
       " 'the',\n",
       " 'site',\n",
       " 'by',\n",
       " 'attaching',\n",
       " 'files',\n",
       " 'to',\n",
       " 'pages',\n",
       " 'http',\n",
       " 'www',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'sites',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'py',\n",
       " 'hl',\n",
       " 'en',\n",
       " 'answer',\n",
       " '90563',\n",
       " 'on',\n",
       " 'the',\n",
       " 'site',\n",
       " 'If',\n",
       " 'you',\n",
       " 're',\n",
       " 'just',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'a',\n",
       " 'place',\n",
       " 'to',\n",
       " 'upload',\n",
       " 'your',\n",
       " 'files',\n",
       " 'so',\n",
       " 'that',\n",
       " 'your',\n",
       " 'group',\n",
       " 'members',\n",
       " 'can',\n",
       " 'download',\n",
       " 'them',\n",
       " 'we',\n",
       " 'suggest',\n",
       " 'you',\n",
       " 'try',\n",
       " 'Google',\n",
       " 'Docs',\n",
       " 'You',\n",
       " 'can',\n",
       " 'upload',\n",
       " 'files',\n",
       " 'http',\n",
       " 'docs',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'py',\n",
       " 'hl',\n",
       " 'en',\n",
       " 'answer',\n",
       " '50092',\n",
       " 'and',\n",
       " 'share',\n",
       " 'access',\n",
       " 'with',\n",
       " 'either',\n",
       " 'a',\n",
       " 'group',\n",
       " 'http',\n",
       " 'docs',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'py',\n",
       " 'hl',\n",
       " 'en',\n",
       " 'answer',\n",
       " '66343',\n",
       " 'or',\n",
       " 'an',\n",
       " 'individual',\n",
       " 'http',\n",
       " 'docs',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'py',\n",
       " 'hl',\n",
       " 'en',\n",
       " 'answer',\n",
       " '86152',\n",
       " 'assigning',\n",
       " 'either',\n",
       " 'edit',\n",
       " 'or',\n",
       " 'download',\n",
       " 'only',\n",
       " 'access',\n",
       " 'to',\n",
       " 'the',\n",
       " 'files',\n",
       " 'you',\n",
       " 'have',\n",
       " 'received',\n",
       " 'this',\n",
       " 'mandatory',\n",
       " 'email',\n",
       " 'service',\n",
       " 'announcement',\n",
       " 'to',\n",
       " 'update',\n",
       " 'you',\n",
       " 'about',\n",
       " 'important',\n",
       " 'changes',\n",
       " 'to',\n",
       " 'Google',\n",
       " 'Groups',\n",
       " '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now letâ€™s see this in action with a full email from our email dataset.\n",
    "with open(r'email\\ham\\6.txt') as f:\n",
    "    # let's split it using regular expressions\n",
    "    listofTokens = re.split('\\W+', f.read())\n",
    "listofTokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the naive bayes classifier again\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabList = set([])\n",
    "    for document in dataSet:\n",
    "        vocabList = vocabList | set(document)\n",
    "    return list(vocabList)\n",
    "\n",
    "def bagofWords2Vector(vocabList, inputDoc):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputDoc:\n",
    "        try:\n",
    "            index = vocabList.index(word)\n",
    "        except ValueError:\n",
    "            print(\"The word {} is not contained in the vocabList\".format(word))\n",
    "        else:\n",
    "            returnVec[index] += 1\n",
    "    return np.array(returnVec)\n",
    "\n",
    "def trainNB(trainMatrix, categoryList):\n",
    "    numofWords = len(trainMatrix[0])\n",
    "    numofDocuments = len(trainMatrix)\n",
    "    p1Num = np.ones(numofWords); p0Num = np.ones(numofWords)\n",
    "    p1Denom = 2.0; p0Denom = 2.0\n",
    "    for i in range(numofDocuments):\n",
    "        if categoryList[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix)\n",
    "        elif categoryList[i] == 0:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix)\n",
    "    p1Vec = np.log(p1Num/p1Denom)\n",
    "    p0Vec = np.log(p0Num/p0Denom)\n",
    "    pSpam = sum(categoryList)/numofDocuments\n",
    "    return p1Vec, p0Vec, pSpam\n",
    "\n",
    "def classifyNB(vector2classify, p1Vec, p0Vec, pSpam):\n",
    "    pNotSpam = 1-pSpam\n",
    "    p1 = sum(vector2classify * p1Vec) + np.log(pSpam)\n",
    "    p0 = sum(vector2classify * p0Vec) + np.log(pNotSpam)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "# This function takes a big string and parses out the text into a list of strings.\n",
    "# It eliminates anything under two characters long and converts everything to lowercase.\n",
    "def textParse(bigString):\n",
    "    listofTokens = [x for x in re.split('\\W+', bigString) if len(x)>2]\n",
    "    return listofTokens\n",
    "\n",
    "# this function automates the naive bayes classifier\n",
    "# and calculates and returns the error rate\n",
    "def spamTest():\n",
    "    # wordlist is for temporarily storing the read files (or let's call them documents)\n",
    "    # docList is for storing the documents\n",
    "    # categoryList stores the document's category (whether spam or not spam)\n",
    "    docList=[]; fullList=[]; categoryList=[]\n",
    "    for i in range(1, 26): # because there are 25 files\n",
    "        with open('email/spam/%d.txt' % i) as f:\n",
    "            wordList = textParse(f.read())\n",
    "        docList.append(wordList)\n",
    "        categoryList.append(1)\n",
    "        with open('email/ham/%d.txt' % i) as f:\n",
    "            wordList = textParse(f.read())\n",
    "        docList.append(wordList)\n",
    "        categoryList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    # we'll train the classifier on 40 samples and then test it on the remaining 10 samples\n",
    "    # let's create the training and testing set\n",
    "    trainSet = np.arange(50)\n",
    "    testSet=np.random.choice(trainSet, 10)\n",
    "    trainSet = np.delete(trainSet, testSet)\n",
    "    # Now let's create the training matrix and train Category list\n",
    "    trainMat = []; trainCatList=[]\n",
    "    for i in trainSet:\n",
    "        trainMat.append(bagofWords2Vector(vocabList, docList[i]))\n",
    "        trainCatList.append(categoryList[i])\n",
    "    p0Vec, p1Vec, pSpam = trainNB(trainMat, trainCatList)\n",
    "    # now let's calculate the error rate\n",
    "    errorCount=0 # it increases by 1 nif there's an error\n",
    "    for i in testSet:\n",
    "        wordVec = bagofWords2Vector(vocabList, docList[i])\n",
    "        if categoryList[i] != classifyNB(wordVec, p0Vec, p1Vec, pSpam):\n",
    "            errorCount += 1\n",
    "    errorRatio = errorCount/len(testSet)\n",
    "    return float(errorRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EMS', 'MBA', 'Microsoft', 'price', 'prototype', 'dusty', 'your', 'Just', 'two', 'PERMANANTLY', 'pavilion', 'message', 'reliever', 'These', 'issues', 'starting', 'inspired', 'computing', 'keep', 'art', 'Check', 'Could', 'enjoy', 'hotels', 'Sorry', 'yeah', '100', 'softwares', 'only', '5mg', 'because', 'Brands', 'welcome', 'Of_PenisEn1argement', 'designed', 'pill', 'VISA', 'there', 'finance', 'Sounds', 'Wilmott', 'Safest', 'comment', 'then', 'Zolpidem', 'plugin', 'UPS', 'foaming', 'model', 'fine', 'sites', 'titles', 'meet', 'view', 'too', 'page', 'signed', 'guy', 'Most', 'Shipment', 'follow', 'Brand', 'access', 'others', 'spaying', 'fans', 'Cost', 'retirement', '750', 'right', 'latest', 'NaturalPenisEnhancement', 'All', 'year', 'recieve', 'let', 'serial', 'risk', 'Experts', 'Amazing', 'butt', 'Can', 'Benoit', 'either', 'below', 'http', 'china', 'extended', 'Mandelbrot', 'supporting', '138', 'edit', 'Codeine', 'OEM', 'sky', 'level', 'Millions', '291', 'job', '588', 'York', 'MoneyBack', 'place', 'doggy', 'school', 'copy', 'website', 'train', 'storage', 'Major', 'done', '219', 'find', 'Today', 'source', 'derivatives', '0nline', 'features', 'service', '430', 'withoutPrescription', 'FreeViagra', 'specifications', 'back', 'www', 'required', 'Chinese', 'logged', 'going', 'moderately', 'definitely', 'analgesic', 'Hydrocodone', 'BrandViagra', 'Have', 'chance', 'more', 'faster', 'color', 'ideas', 'via', 'Troy', 'holiday', 'yourPenis', 'pick', 'forum', '562', 'Experience', 'ofEjacu1ate', 'Credit', 'Accepted', 'Does', 'Julius', 'rude', 'Pharmacy', '292', 'Please', '25mg', 'Sure', 'reputable', 'died', 'car', 'Perhaps', '2011', 'far', 'brands', 'From', 'often', 'Quality', 'capabilities', '225', 'google', 'father', 'thickness', 'station', 'computer', 'party', 'development', '100M', 'Year', 'contact', 'might', 'take', 'The', 'Required', 'Everything', 'trip', 'Reply', 'bike', 'located', 'Carlo', 'Google', '50mg', 'talked', 'lunch', 'Hommies', 'Success', 'store', 'leaves', '366', 'BiggerPenis', 'Eugene', 'They', 'AMEX', 'effective', 'encourage', 'products', 'also', 'rent', 'mathematician', 'saw', 'close', 'program', 'made', 'book', 'needed', 'Grow', 'lists', 'expo', 'Speedpost', 'OrderCializViagra', 'top', 'about', '66343', 'wasn', 'LinkedIn', 'Jocelyn', 'hangzhou', 'WatchesStore', 'Tiffany', 'bad', 'Ryan', 'class', 'Take', 'courier', 'approach', 'launch', 'Photoshop', '2010', 'dozen', 'Professional', 'has', 'accept', 'writing', 'treat', 'money', 'automatic', 'specifically', 'cats', 'creation', 'members', 'free', 'Vicodin', 'WARRANTY', 'Gucci', 'having', 'riding', 'FedEx', 'Methylmorphine', 'pages', 'working', 'knocking', 'but', 'get', 'professional', 'just', 'CS5', '625', 'couple', 'WILSON', 'thing', 'care', 'groups', 'tour', 'Stepp', 'not', 'behind', 'Plus', 'door', 'WHat', 'length', 'Learn', 'Dior', 'what', 'connection', 'Since', 'life', 'jpgs', 'turd', 'Fast', 'Watson', 'significantly', 'ones', 'decision', 'For', 'SeverePain', 'town', '492', 'today', 'Gain', 'would', 'Jar', 'thread', 'SciFinance', 'should', 'online', 'window', 'such', 'them', 'great', 'increase', 'inconvenience', 'quantitative', 'Strategy', 'know', 'winter', 'mandatory', 'don', 'here', 'drunk', 'Only', 'upload', '195', 'Pro', 'docs', 'they', 'glimpse', '120', 'FBI', 'financial', 'name', 'drugs', 'opioid', 'note', 'away', 'owner', 'Doctor', 'Certified', 'things', 'expertise', 'cannot', 'mailing', 'must', 'horn', 'jqplot', 'Ultimate', 'management', 'when', 'differ', 'Private', 'that', 'food', 'day', 'Docs', 'lined', 'cards', 'Hermes', 'thank', 'DHL', 'Looking', 'narcotic', 'Genuine', '180', 'Worldwide', 'Here', 'ferguson', 'invitation', 'Guaranteeed', 'focus', 'This', 'email', 'You', 'placed', 'NoPrescription', 'tent', 'Watches', '174623', 'prices', 'hope', 'past', 'Drugs', 'major', 'Cartier', 'hours', 'generation', 'Save', 'Superb', '14th', 'you', 'from', 'Discreet', 'huge', 'Vuitton', 'Wholesale', 'Yay', '50092', 'concise', 'release', 'any', 'brained', 'February', 'work', 'Zach', 'those', 'fractal', 'nature', 'aged', 'cat', 'Acrobat', 'knew', 'income', 'Sites', 'New', 'Louis', 'control', 'advocate', 'help', 'running', 'reservation', 'coast', 'credit', 'Bargains', 'heard', 'being', '385', 'for', '129', 'functionalities', 'much', 'once', 'CHECK', '322', 'Python', 'FDA', 'Trusted', 'Vivek', 'Safe', 'assigning', 'Thirumalai', 'Order', 'mail', 'endorsed', '1924', 'both', 'important', 'address', 'file', 'order', 'new', 'need', 'OFF', 'That', 'share', 'add', 'Thailand', 'over', 'create', 'had', 'style', 'Home', 'survive', 'mathematics', 'Are', 'possible', 'have', 'way', 'Tokyo', 'Fermi', 'think', 'Thank', 'windows', 'incoming', 'this', 'using', 'Thanks', 'came', 'been', 'who', 'CUDA', 'shape', 'Jose', 'business', 'earn', '15mg', '130', 'Whybrew', 'Your', 'thought', 'There', 'looking', '325', 'Explosive', 'high', 'Supplement', 'BetterErections', 'now', 'tickets', 'days', 'individual', 'inform', 'NET', 'how', 'changing', 'Hello', 'runs', 'favorite', 'longer', 'please', 'discussions', 'good', 'game', 'province', 'item', 'ViagraNoPrescription', 'will', 'assistance', 'told', '513', 'pain', 'October', 'status', 'Jewerly', 'View', 'museum', 'hold', 'night', 'volume', 'exhibit', 'location', 'changes', 'went', 'FREE', 'plane', 'full', 'easily', 'each', 'Bags', 'bathroom', 'information', 'creative', 'Tesla', 'tool', 'cold', 'customized', 'jquery', 'Cheers', 'web', 'Phentermin', 'Haloney', 'Discount', 'all', 'attaching', 'gains', 'Magazine', 'some', 'BetterEjacu1ation', 'with', 'Groups', 'doing', 'magazine', 'Arvind', 'Will', 'enabled', 'EXPRESS', 'yesterday', '203', 'notification', 'Where', 'Finder', 'Mandarin', 'group', 'strategic', 'same', 'most', 'How', 'improving', 'Kerry', 'answer', 'Team', 'selected', 'Jay', 'StoreDetailView_98', 'one', 'John', 'With', '200', 'wednesday', 'Express', '90563', 'bin', 'call', 'while', 'Germany', 'grounds', 'may', 'code', 'competitive', 'could', 'Delivery', 'want', '156', 'suggest', 'and', 'tabs', 'featured', 'moderate', 'sophisticated', 'Top', 'try', '199', 'said', '100mg', 'BuyVIAGRA', 'held', 'insights', 'ready', 'pills', 'HardErecetions', 'check', 'network', 'opportunity', 'interesting', 'Hamm', 'Cheap', 'Low', 'programming', 'Series', 'use', 'Effective', 'was', 'Don', 'requested', 'automatically', 'inside', 'Instead', 'his', 'Increase', 'Adobe', 'FEDEX', 'Monte', 'sliding', 'enough', 'Work', 'home', 'announcement', 'Design', 'link', 'these', 'well', 'CCA', 'used', 'are', 'Approved', 'Get', 'pricing', 'Oris', 'Ambiem', 'Rock', 'time', 'com', 'GPU', 'uses', 'Buy', 'files', '570', 'focusing', 'doors', 'includes', 'support', 'rain', '86152', '10mg', 'blue', 'Let', 'Methods', 'Ma1eEnhancement', 'Incredib1e', 'Extended', 'through', 'parallel', 'herbal', 'articles', 'Giants', 'listed', 'forward', 'Prices', 'Percocet', 'questions', 'mom', 'Natural', 'Peter', 'the', 'core', 'girl', '396', 'Famous', 'borders', 'scenic', 'pls', 'thousand', 'stuff', 'pictures', 'Enjoy', 'pretty', 'NVIDIA', 'Office', 'hotel', 'out', '2007', 'download', 'inches', 'modelling', 'example', 'per', 'FemaleViagra', 'based', '30mg', '300x', 'number', 'items', 'Regards', 'site', 'chapter', 'can', 'aRolexBvlgari', 'proven', 'Accept', 'least', 'wrote', 'phone', 'Cards', 'come', 'intenseOrgasns', 'another', 'reply', 'Shipping', 'than', 'Canadian', 'got', 'Online', 'update', 'Wallets', 'roofer', 'sent', 'prepared', 'fundamental', 'gas', 'generates', 'Windows', 'like', 'commented', '119', 'transformed', 'received', 'see', 'works'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
