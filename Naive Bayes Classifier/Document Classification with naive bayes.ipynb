{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Problem:** \n",
    "letâ€™s make a quick filter for an online message board that flags\n",
    "a message as inappropriate if the author uses negative or abusive language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prepare: making word vectors from text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates some example data to experiment with. This is the training data\n",
    "# The first variable returned from loadDatSet() is a list of documents from a Dog lovers message board.\n",
    "# The text has been broken up into a set of tokens (words)\n",
    "# Punctuation has been removed from this text as well.\n",
    "\n",
    "# The second variable of loadDatSet() returns a set of class labels.\n",
    "# Here you have two classes, abusive and not abusive.\n",
    "# The text has been labeled by a human and will be used to train a program to automatically detect abusive posts.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def loadDataSet():\n",
    "    documentVec=[['my', 'dog', 'has', 'flea', # document - list of tokens or words\n",
    "    'problems', 'help', 'please'],\n",
    "    ['maybe', 'not', 'take', 'him',\n",
    "    'to', 'dog', 'park', 'stupid'],\n",
    "    ['my', 'dalmation', 'is', 'so', 'cute',\n",
    "    'I', 'love', 'him'],\n",
    "    ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "    ['mr', 'licks', 'ate', 'my', 'steak', 'how',\n",
    "    'to', 'stop', 'him'],\n",
    "    ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    # 6 labels for 6 messages\n",
    "    classVec = [0,1,0,1,0,1] #1 is abusive, 0 not\n",
    "    return documentVec,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will create a list of all the unique words from all of our documents.\n",
    "# vocabulary list is the list of all the words youâ€™d like to examine and create a feature for each of them.\n",
    "\n",
    "def createVocabList(dataSet): # here, dataset should be a list of lists\n",
    "    vocabSet = set([]) # set only consists of unique numbers\n",
    "    for document in dataSet:\n",
    "        # append the set with a new set from each document.\n",
    "        # | stands for union of two tests.  \n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the vocabulary list and a document and outputs a vector\n",
    "# of 1s and 0s to represent whether a word from our vocabulary is present or not in the given document.\n",
    "\n",
    "# first create a vector the same length as the vocabulary list and fill it up with 0s.\n",
    "# and Next, go through the words in the document, and if the word\n",
    "# is in the vocabulary list, you set its value to 1 in the output vector.\n",
    "\n",
    "def bagOfWords2Vec(vocabList, tokenList):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in tokenList:\n",
    "        try:\n",
    "            index = vocabList.index(word)\n",
    "        except ValueError:\n",
    "            print(\"the word: {} is not in my Vocabulary!\".format(word))\n",
    "        else:\n",
    "            returnVec[index] += 1 # bag of words model\n",
    "            \n",
    "    return np.array(returnVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train: calculating probabilities from word vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "letâ€™s see how to calculate the probabilities with these numbers. You know whether a word occurs in a document,\n",
    "and you know what class the document belongs to.\n",
    "\n",
    "Let $w$ represent the vector of words. $w$ = $[w_{0}, w_{1}, w_{2}, w_{3}, ...]$\n",
    "<br>now, we need to calculate the probability of a message being abusive or not abusive (probability that it belongs to a particular class) given that it contains a particular vector of words.\n",
    "<br> or $P(class_{i}|w)$\n",
    "\n",
    "By Bayes theorem,\n",
    "<br>$P(class_{i}|w)$ = $\\dfrac {P(w|class_{i})\\ * P(class_{i})}{P(w)}$\n",
    "\n",
    "Now, $P(class_{i})$ can be calculated as, $\\dfrac{Number\\ of\\ times\\ class_{i}\\ has\\ been\\ reported}{Total\\ number\\ of\\ reports}$\n",
    "\n",
    "$P(w/class_{i})$ is the probability of the particular vector of words occuring given that it's belongs to $class_{i}$ \n",
    "<br>How can we calculate this? This is where our naÃ¯ve assumption comes in. \n",
    "<br>If we expand $w$ into\n",
    "individual features, we could rewrite this as $p(w0,w1,w2..wN|ci)$. \n",
    "<br>Our assumption that\n",
    "all the words were independently likely, and something called conditional independence,\n",
    "says we can calculate this probability as $p(w0|ci)\\ *\\ p(w1|ci)\\ *\\ p(w2|ci)\\ ...p(wN|ci)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes in a list of word vectors(vector of 1s and 0s) and their corresponding category list\n",
    "# and returns a list of their conditional probabilities\n",
    "# i.e [P(w0|ci) P(w1|ci) P(w2|ci) P(w3|ci) P(w4|ci) .......]\n",
    "\n",
    "# we get this by adding all the word vectors for each category,\n",
    "# Now the addendum list contains the count of words\n",
    "# we divide this by the total count (summing the addendum) to get the conditional probability\n",
    "\n",
    "# trainMatrix - matrix (list) of documents\n",
    "# trainCategory - corresponding list of categories\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix) # number of documents\n",
    "    numWords = len(trainMatrix[0]) # number of words in each word vector (the length of vocabulary list)\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs) # probability of Abusive class\n",
    "    \n",
    "    # np.zeroes creates an array of zeroes of given size passed as an argument\n",
    "    # The numerator is a NumPy array of zeros with length same as the word vector\n",
    "    # In the for loop we loop over all the documents in trainMatrix, or our training set. \n",
    "    # Every time a word appears in a document, the count for that word (p1Num or p0Num) gets incremented, \n",
    "    # and the total number of words for a document gets summed up over all the documents.\n",
    "    \n",
    "    # Now to calculate the conditional probability for a given class\n",
    "    # We have to multiply all the values in the probability vector except 0\n",
    "    # the problem is if we multiply small values in python, it'll become smaller and eventually winds up to 0\n",
    "    # hence we convert them log values and add them\n",
    "    # like log(a*b) = log(a) + log(b)\n",
    "    # natural log of a function can be used in place of a function when youâ€™re interested\n",
    "    # in finding the maximum value of that function.\n",
    "    \n",
    "    # To convert to log function, we've to initialize the numerator with ones(since log(0)=inf) and denominators with 2\n",
    "    # there isn't an exact science, but generally when you increment the numerator with 1,\n",
    "    # you'd have to increment the denominator with 2 to get almost the same ratio\n",
    "    \n",
    "    p0Num = np.ones(numWords); p1Num = np.ones(numWords) # p0 - Class0, p0Num - p0 numerator\n",
    "    p0Denom = 2.0; p1Denom = 2.0 # the denominator is initialized by twos\n",
    "    for i in range(numTrainDocs): # p1-abusive, p0-notabusive\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])      \n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num/p1Denom) # we convert them into log\n",
    "    p0Vect = np.log(p0Num/p0Denom) \n",
    "    pNotAbusive = 1 - pAbusive\n",
    "    return p0Vect,p1Vect,pAbusive, pNotAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.04452244, -3.04452244, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -3.04452244, -1.94591015,\n",
       "       -2.35137526, -3.04452244, -2.35137526, -2.35137526, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,\n",
       "       -3.04452244, -1.65822808, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -2.35137526, -1.94591015, -2.35137526, -2.35137526, -3.04452244,\n",
       "       -2.35137526, -2.35137526])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMatrix = []\n",
    "for i in doclist:\n",
    "    trainMatrix.append(bagOfWords2Vec(vocablist, i))\n",
    "p0Vec, p1Vec, class1, class0 = trainNB0(trainMatrix, categorylist)\n",
    "p1Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So up until this point, we've calculated the conditional probability vectors i.e P(w|c) from the huge training set\n",
    "# what they mean is, the probability of each feature given that they occur in the population of c\n",
    "\n",
    "# In the Naive Bayes Classifier, we'll be given a new document to classify\n",
    "# the probability that the document belongs to classi is ð‘ƒ(ð‘ð‘™ð‘Žð‘ ð‘ ð‘–|ð‘¤) = ð‘ƒ(ð‘¤|ð‘ð‘™ð‘Žð‘ ð‘ ð‘–) âˆ—ð‘ƒ(ð‘ð‘™ð‘Žð‘ ð‘ ð‘–)/ð‘ƒ(ð‘¤)\n",
    "\n",
    "# First we need to calculate ð‘ƒ(ð‘¤|ð‘ð‘™ð‘Žð‘ ð‘ ð‘–)\n",
    "# we know (or atleast assume) that the probability of the independent tokens (features or words)\n",
    "# is already present in the probability vector\n",
    "# hence we extract the required probabilities from the vector and compute P(w|classi)\n",
    "\n",
    "# we already computed P(classi) from the training set\n",
    "\n",
    "# We don't need to calculate P(w)\n",
    "# since we're essentially comparing the probabilities of classes here and P(w) gets cancelled\n",
    "\n",
    "# let's build the Naive Bayes Classifier\n",
    "\n",
    "def classifyNB(vector2classify, p0Vec, p1Vec, class1, class0):\n",
    "    p1 = sum(vector2classify * p1Vec) + np.log(class1) # extract the probabilities\n",
    "    p0 = sum(vector2classify * p0Vec) + np.log(class0) # and logarithmically add to P(class)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's put 'em all together\n",
    "def testNB(document2classify):\n",
    "    trainDocuments, trainClass = loadDataSet()\n",
    "    vocabList = createVocabList(trainDocuments)\n",
    "    trainDocumentMatrix = []\n",
    "    for i in range(len(trainDocuments)):\n",
    "        trainDocumentMatrix.append(bagOfWords2Vec(vocabList, trainDocuments[i]))\n",
    "    testDocVec = bagOfWords2Vec(vocabList, document2classify)\n",
    "    p0Vec, p1Vec, class1, class0 = trainNB0(trainDocumentMatrix, trainClass)\n",
    "    return classifyNB(testDocVec, p0Vec, p1Vec, class1, class0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBclassifier(document2classify):\n",
    "    x = testNB(document2classify)\n",
    "    if x ==1:\n",
    "        print(\"This document is abusive\")\n",
    "    elif x ==0:\n",
    "        print(\"This document is not abusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is abusive\n"
     ]
    }
   ],
   "source": [
    "NBclassifier([\"stupid\", \"garbage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
